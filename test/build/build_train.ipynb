{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from attentional_cpmp.model import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 7\n",
    "key_dim = H + 1\n",
    "value_dim = None\n",
    "num_heads = 3\n",
    "num_stacks = 3\n",
    "list_neurons_hide = [H+1, H+1]\n",
    "list_neurons_feed = [H+1, (H+1)*4, (H+1)*3, (H+1)*2]\n",
    "dropout = 0\n",
    "rate = 0.2\n",
    "activation_hide = 'sigmoid'\n",
    "activation_feed = 'sigmoid'\n",
    "n_dropout_hide = 0\n",
    "n_dropout_feed = 2\n",
    "\n",
    "model_x = create_model(H=H,\n",
    "                     key_dim=key_dim,\n",
    "                     value_dim=value_dim,\n",
    "                     num_heads=num_heads,\n",
    "                     num_stacks=num_stacks,\n",
    "                     list_neurons_hide=list_neurons_hide,\n",
    "                     list_neurons_feed=list_neurons_feed,\n",
    "                     dropout=dropout,\n",
    "                     rate=rate,\n",
    "                     activation_hide=activation_hide,\n",
    "                     activation_feed=activation_feed,\n",
    "                     n_dropout_hide=n_dropout_hide,\n",
    "                     n_dropout_feed=n_dropout_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attentional_cpmp.utils.data_saving.data_json import load_data_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Sx7 = load_data_from_json(file_path=\"CPMP_With_Attention.Sx7_v4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "claves_a_incluir = ['5', '6', '7']\n",
    "\n",
    "# Crear subdiccionario\n",
    "sub_data_Sx7 = dict((clave, data_Sx7[clave]) for clave in claves_a_incluir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['5', '6', '7'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_data_Sx7.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1973/1973 [==============================] - 55s 20ms/step - loss: 0.1958 - mae: 0.1158 - mse: 0.0574\n",
      "Epoch 2/10\n",
      " 178/1973 [=>............................] - ETA: 40s - loss: 0.1711 - mae: 0.1031 - mse: 0.0514"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stack \u001b[38;5;129;01min\u001b[39;00m sub_data_Sx7:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmodel_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_Sx7\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_Sx7\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for stack in sub_data_Sx7:\n",
    "    model_x.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 46.1\n",
      "mean steps: 57.67245119305857\n",
      "median steps: 58.0\n",
      "min steps: 40.0\n",
      "max steps: 86.0\n",
      "\n",
      "success heuristic (%): 97.1 55.53038105046344\n",
      "mean steps: 55.53038105046344\n",
      "median steps: 55.0\n",
      "min steps: 36.0\n",
      "max steps: 79.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46.1, 97.1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cpmp_ml.validations import validate_model\n",
    "from cpmp_ml.optimizer import GreedyV2\n",
    "from cpmp_ml.utils.adapters import AttentionModel\n",
    "\n",
    "validate_model(model, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = create_model(H=H,\n",
    "                     key_dim=key_dim,\n",
    "                     value_dim=value_dim,\n",
    "                     num_heads=num_heads,\n",
    "                     num_stacks=num_stacks,\n",
    "                     list_neurons_hide=list_neurons_hide,\n",
    "                     list_neurons_feed=list_neurons_feed,\n",
    "                     dropout=dropout,\n",
    "                     rate=rate,\n",
    "                     activation_hide=activation_hide,\n",
    "                     activation_feed=activation_feed,\n",
    "                     n_dropout_hide=n_dropout_hide,\n",
    "                     n_dropout_feed=n_dropout_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1973/1973 [==============================] - 57s 21ms/step - loss: 0.1873 - mae: 0.1104 - mse: 0.0547\n",
      "Epoch 2/10\n",
      "1973/1973 [==============================] - 42s 21ms/step - loss: 0.1651 - mae: 0.0991 - mse: 0.0494\n",
      "Epoch 3/10\n",
      "1973/1973 [==============================] - 42s 21ms/step - loss: 0.1621 - mae: 0.0973 - mse: 0.0485\n",
      "Epoch 4/10\n",
      "1973/1973 [==============================] - 46s 23ms/step - loss: 0.1606 - mae: 0.0964 - mse: 0.0481\n",
      "Epoch 5/10\n",
      "1973/1973 [==============================] - 39s 20ms/step - loss: 0.1595 - mae: 0.0958 - mse: 0.0478\n",
      "Epoch 6/10\n",
      "1973/1973 [==============================] - 42s 21ms/step - loss: 0.1587 - mae: 0.0953 - mse: 0.0476\n",
      "Epoch 7/10\n",
      "1973/1973 [==============================] - 42s 21ms/step - loss: 0.1581 - mae: 0.0950 - mse: 0.0474\n",
      "Epoch 8/10\n",
      "1973/1973 [==============================] - 44s 22ms/step - loss: 0.1577 - mae: 0.0948 - mse: 0.0473\n",
      "Epoch 9/10\n",
      "1973/1973 [==============================] - 45s 23ms/step - loss: 0.1572 - mae: 0.0944 - mse: 0.0472\n",
      "Epoch 10/10\n",
      "1973/1973 [==============================] - 39s 20ms/step - loss: 0.1568 - mae: 0.0942 - mse: 0.0470\n",
      "Epoch 1/10\n",
      "2327/2327 [==============================] - 70s 27ms/step - loss: 0.1372 - mae: 0.0807 - mse: 0.0404\n",
      "Epoch 2/10\n",
      "2327/2327 [==============================] - 64s 27ms/step - loss: 0.1363 - mae: 0.0802 - mse: 0.0401\n",
      "Epoch 3/10\n",
      "2327/2327 [==============================] - 63s 27ms/step - loss: 0.1357 - mae: 0.0799 - mse: 0.0400\n",
      "Epoch 4/10\n",
      "2327/2327 [==============================] - 63s 27ms/step - loss: 0.1353 - mae: 0.0796 - mse: 0.0398\n",
      "Epoch 5/10\n",
      "2327/2327 [==============================] - 62s 27ms/step - loss: 0.1349 - mae: 0.0795 - mse: 0.0398\n",
      "Epoch 6/10\n",
      "2327/2327 [==============================] - 63s 27ms/step - loss: 0.1346 - mae: 0.0793 - mse: 0.0397\n",
      "Epoch 7/10\n",
      "2327/2327 [==============================] - 62s 27ms/step - loss: 0.1343 - mae: 0.0792 - mse: 0.0396\n",
      "Epoch 8/10\n",
      "2327/2327 [==============================] - 65s 28ms/step - loss: 0.1341 - mae: 0.0790 - mse: 0.0395\n",
      "Epoch 9/10\n",
      "2327/2327 [==============================] - 66s 29ms/step - loss: 0.1338 - mae: 0.0789 - mse: 0.0394\n",
      "Epoch 10/10\n",
      "2327/2327 [==============================] - 60s 26ms/step - loss: 0.1335 - mae: 0.0788 - mse: 0.0394\n",
      "Epoch 1/10\n",
      "2558/2558 [==============================] - 89s 35ms/step - loss: 0.1175 - mae: 0.0674 - mse: 0.0337\n",
      "Epoch 2/10\n",
      "2558/2558 [==============================] - 79s 31ms/step - loss: 0.1170 - mae: 0.0671 - mse: 0.0336\n",
      "Epoch 3/10\n",
      "2558/2558 [==============================] - 77s 30ms/step - loss: 0.1167 - mae: 0.0669 - mse: 0.0335\n",
      "Epoch 4/10\n",
      "2558/2558 [==============================] - 76s 30ms/step - loss: 0.1164 - mae: 0.0668 - mse: 0.0334\n",
      "Epoch 5/10\n",
      "2558/2558 [==============================] - 78s 31ms/step - loss: 0.1162 - mae: 0.0667 - mse: 0.0334\n",
      "Epoch 6/10\n",
      "2558/2558 [==============================] - 76s 30ms/step - loss: 0.1161 - mae: 0.0666 - mse: 0.0333\n",
      "Epoch 7/10\n",
      "2558/2558 [==============================] - 78s 30ms/step - loss: 0.1159 - mae: 0.0665 - mse: 0.0333\n",
      "Epoch 8/10\n",
      "2558/2558 [==============================] - 76s 30ms/step - loss: 0.1157 - mae: 0.0665 - mse: 0.0332\n",
      "Epoch 9/10\n",
      "2558/2558 [==============================] - 78s 30ms/step - loss: 0.1156 - mae: 0.0664 - mse: 0.0332\n",
      "Epoch 10/10\n",
      "2558/2558 [==============================] - 76s 30ms/step - loss: 0.1154 - mae: 0.0663 - mse: 0.0331\n"
     ]
    }
   ],
   "source": [
    "for stack in sub_data_Sx7:\n",
    "    model_2.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 27.0\n",
      "mean steps: 58.28888888888889\n",
      "median steps: 58.0\n",
      "min steps: 39.0\n",
      "max steps: 78.0\n",
      "\n",
      "success heuristic (%): 97.89999999999999 56.25229826353422\n",
      "mean steps: 56.25229826353422\n",
      "median steps: 56.0\n",
      "min steps: 36.0\n",
      "max steps: 83.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(27.0, 97.89999999999999)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(model_2, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 7\n",
    "key_dim = 32\n",
    "value_dim = None\n",
    "num_heads = 3\n",
    "num_stacks = 3\n",
    "list_neurons_hide = [H+1, H+1]\n",
    "list_neurons_feed = [H+1, (H+1)*4, (H+1)*3, (H+1)*2]\n",
    "dropout = 0\n",
    "rate = 0.2\n",
    "activation_hide = 'sigmoid'\n",
    "activation_feed = 'sigmoid'\n",
    "n_dropout_hide = 0\n",
    "n_dropout_feed = 2\n",
    "\n",
    "model_3 = create_model(H=H,\n",
    "                     key_dim=key_dim,\n",
    "                     value_dim=value_dim,\n",
    "                     num_heads=num_heads,\n",
    "                     num_stacks=num_stacks,\n",
    "                     list_neurons_hide=list_neurons_hide,\n",
    "                     list_neurons_feed=list_neurons_feed,\n",
    "                     dropout=dropout,\n",
    "                     rate=rate,\n",
    "                     activation_hide=activation_hide,\n",
    "                     activation_feed=activation_feed,\n",
    "                     n_dropout_hide=n_dropout_hide,\n",
    "                     n_dropout_feed=n_dropout_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1973/1973 [==============================] - 68s 27ms/step - loss: 0.1866 - mae: 0.1107 - mse: 0.0550\n",
      "Epoch 2/10\n",
      "1973/1973 [==============================] - 56s 29ms/step - loss: 0.1665 - mae: 0.0999 - mse: 0.0499\n",
      "Epoch 3/10\n",
      "1973/1973 [==============================] - 56s 28ms/step - loss: 0.1631 - mae: 0.0979 - mse: 0.0488\n",
      "Epoch 4/10\n",
      "1973/1973 [==============================] - 56s 29ms/step - loss: 0.1613 - mae: 0.0968 - mse: 0.0483\n",
      "Epoch 5/10\n",
      "1973/1973 [==============================] - 57s 29ms/step - loss: 0.1604 - mae: 0.0962 - mse: 0.0481\n",
      "Epoch 6/10\n",
      "1973/1973 [==============================] - 57s 29ms/step - loss: 0.1592 - mae: 0.0955 - mse: 0.0477\n",
      "Epoch 7/10\n",
      "1973/1973 [==============================] - 57s 29ms/step - loss: 0.1585 - mae: 0.0952 - mse: 0.0476\n",
      "Epoch 8/10\n",
      "1973/1973 [==============================] - 58s 29ms/step - loss: 0.1578 - mae: 0.0948 - mse: 0.0474\n",
      "Epoch 9/10\n",
      "1973/1973 [==============================] - 58s 30ms/step - loss: 0.1571 - mae: 0.0944 - mse: 0.0472\n",
      "Epoch 10/10\n",
      "1973/1973 [==============================] - 58s 29ms/step - loss: 0.1567 - mae: 0.0942 - mse: 0.0471\n",
      "Epoch 1/10\n",
      "2327/2327 [==============================] - 92s 36ms/step - loss: 0.1370 - mae: 0.0806 - mse: 0.0403\n",
      "Epoch 2/10\n",
      "2327/2327 [==============================] - 84s 36ms/step - loss: 0.1362 - mae: 0.0802 - mse: 0.0401\n",
      "Epoch 3/10\n",
      "2327/2327 [==============================] - 83s 36ms/step - loss: 0.1357 - mae: 0.0799 - mse: 0.0400\n",
      "Epoch 4/10\n",
      "2327/2327 [==============================] - 83s 36ms/step - loss: 0.1353 - mae: 0.0797 - mse: 0.0399\n",
      "Epoch 5/10\n",
      "2327/2327 [==============================] - 87s 37ms/step - loss: 0.1349 - mae: 0.0795 - mse: 0.0397\n",
      "Epoch 6/10\n",
      "2327/2327 [==============================] - 82s 35ms/step - loss: 0.1345 - mae: 0.0792 - mse: 0.0396\n",
      "Epoch 7/10\n",
      "2327/2327 [==============================] - 82s 35ms/step - loss: 0.1342 - mae: 0.0790 - mse: 0.0395\n",
      "Epoch 8/10\n",
      "2327/2327 [==============================] - 83s 36ms/step - loss: 0.1339 - mae: 0.0789 - mse: 0.0395\n",
      "Epoch 9/10\n",
      "2327/2327 [==============================] - 82s 35ms/step - loss: 0.1337 - mae: 0.0787 - mse: 0.0394\n",
      "Epoch 10/10\n",
      "2327/2327 [==============================] - 83s 36ms/step - loss: 0.1334 - mae: 0.0786 - mse: 0.0393\n",
      "Epoch 1/10\n",
      "2558/2558 [==============================] - 104s 41ms/step - loss: 0.1177 - mae: 0.0674 - mse: 0.0337\n",
      "Epoch 2/10\n",
      "2558/2558 [==============================] - 105s 41ms/step - loss: 0.1172 - mae: 0.0671 - mse: 0.0336\n",
      "Epoch 3/10\n",
      "2558/2558 [==============================] - 103s 40ms/step - loss: 0.1169 - mae: 0.0669 - mse: 0.0335\n",
      "Epoch 4/10\n",
      "2558/2558 [==============================] - 104s 41ms/step - loss: 0.1165 - mae: 0.0668 - mse: 0.0334\n",
      "Epoch 5/10\n",
      "2558/2558 [==============================] - 99s 39ms/step - loss: 0.1163 - mae: 0.0667 - mse: 0.0334\n",
      "Epoch 6/10\n",
      "2558/2558 [==============================] - 99s 39ms/step - loss: 0.1161 - mae: 0.0665 - mse: 0.0333\n",
      "Epoch 7/10\n",
      "2558/2558 [==============================] - 100s 39ms/step - loss: 0.1159 - mae: 0.0665 - mse: 0.0332\n",
      "Epoch 8/10\n",
      "2558/2558 [==============================] - 100s 39ms/step - loss: 0.1157 - mae: 0.0663 - mse: 0.0332\n",
      "Epoch 9/10\n",
      "2558/2558 [==============================] - 97s 38ms/step - loss: 0.1156 - mae: 0.0663 - mse: 0.0332\n",
      "Epoch 10/10\n",
      "2558/2558 [==============================] - 100s 39ms/step - loss: 0.1154 - mae: 0.0662 - mse: 0.0331\n"
     ]
    }
   ],
   "source": [
    "for stack in sub_data_Sx7:\n",
    "    model_3.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 61.9\n",
      "mean steps: 54.919224555735056\n",
      "median steps: 55.0\n",
      "min steps: 39.0\n",
      "max steps: 74.0\n",
      "\n",
      "success heuristic (%): 97.1 55.92584963954686\n",
      "mean steps: 55.92584963954686\n",
      "median steps: 56.0\n",
      "min steps: 34.0\n",
      "max steps: 79.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61.9, 97.1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(model_3, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('model_1.h5')\n",
    "model_2.save('model_2.h5')\n",
    "model_3.save('model_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 7\n",
    "key_dim = H + 1\n",
    "value_dim = None\n",
    "num_heads = 3\n",
    "num_stacks = 3\n",
    "list_neurons_hide = [H+1, H+1]\n",
    "list_neurons_feed = [H+1, (H+1)*4, (H+1)*3, (H+1)*2]\n",
    "dropout = 0\n",
    "rate = 0.2\n",
    "activation_hide = 'sigmoid'\n",
    "activation_feed = 'sigmoid'\n",
    "n_dropout_hide = 0\n",
    "n_dropout_feed = 2\n",
    "\n",
    "model_4 = create_model(H=H,\n",
    "                     key_dim=key_dim,\n",
    "                     value_dim=value_dim,\n",
    "                     num_heads=num_heads,\n",
    "                     num_stacks=num_stacks,\n",
    "                     list_neurons_hide=list_neurons_hide,\n",
    "                     list_neurons_feed=list_neurons_feed,\n",
    "                     dropout=dropout,\n",
    "                     rate=rate,\n",
    "                     activation_hide=activation_hide,\n",
    "                     activation_feed=activation_feed,\n",
    "                     n_dropout_hide=n_dropout_hide,\n",
    "                     n_dropout_feed=n_dropout_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1973/1973 [==============================] - 84s 29ms/step - loss: 0.1942 - mae: 0.1148 - mse: 0.0568\n",
      "Epoch 2/10\n",
      "1973/1973 [==============================] - 54s 28ms/step - loss: 0.1665 - mae: 0.1003 - mse: 0.0500\n",
      "Epoch 3/10\n",
      "1973/1973 [==============================] - 50s 25ms/step - loss: 0.1630 - mae: 0.0980 - mse: 0.0489\n",
      "Epoch 4/10\n",
      "1973/1973 [==============================] - 49s 25ms/step - loss: 0.1612 - mae: 0.0969 - mse: 0.0484\n",
      "Epoch 5/10\n",
      "1973/1973 [==============================] - 44s 22ms/step - loss: 0.1601 - mae: 0.0962 - mse: 0.0480\n",
      "Epoch 6/10\n",
      "1973/1973 [==============================] - 54s 27ms/step - loss: 0.1593 - mae: 0.0958 - mse: 0.0478\n",
      "Epoch 7/10\n",
      "1973/1973 [==============================] - 43s 22ms/step - loss: 0.1585 - mae: 0.0953 - mse: 0.0476\n",
      "Epoch 8/10\n",
      "1973/1973 [==============================] - 44s 22ms/step - loss: 0.1580 - mae: 0.0950 - mse: 0.0474\n",
      "Epoch 9/10\n",
      "1973/1973 [==============================] - 43s 22ms/step - loss: 0.1574 - mae: 0.0946 - mse: 0.0473\n",
      "Epoch 10/10\n",
      "1973/1973 [==============================] - 43s 22ms/step - loss: 0.1571 - mae: 0.0945 - mse: 0.0472\n",
      "Epoch 1/10\n",
      "2327/2327 [==============================] - 68s 26ms/step - loss: 0.1374 - mae: 0.0808 - mse: 0.0404\n",
      "Epoch 2/10\n",
      "2327/2327 [==============================] - 61s 26ms/step - loss: 0.1366 - mae: 0.0804 - mse: 0.0402\n",
      "Epoch 3/10\n",
      "2327/2327 [==============================] - 59s 25ms/step - loss: 0.1361 - mae: 0.0801 - mse: 0.0401\n",
      "Epoch 4/10\n",
      "2327/2327 [==============================] - 61s 26ms/step - loss: 0.1356 - mae: 0.0799 - mse: 0.0400\n",
      "Epoch 5/10\n",
      "2327/2327 [==============================] - 59s 25ms/step - loss: 0.1352 - mae: 0.0797 - mse: 0.0399\n",
      "Epoch 6/10\n",
      "2327/2327 [==============================] - 60s 26ms/step - loss: 0.1348 - mae: 0.0795 - mse: 0.0398\n",
      "Epoch 7/10\n",
      "2327/2327 [==============================] - 66s 28ms/step - loss: 0.1347 - mae: 0.0794 - mse: 0.0397\n",
      "Epoch 8/10\n",
      "2327/2327 [==============================] - 63s 27ms/step - loss: 0.1343 - mae: 0.0793 - mse: 0.0396\n",
      "Epoch 9/10\n",
      "2327/2327 [==============================] - 59s 25ms/step - loss: 0.1341 - mae: 0.0791 - mse: 0.0396\n",
      "Epoch 10/10\n",
      "2327/2327 [==============================] - 60s 26ms/step - loss: 0.1339 - mae: 0.0790 - mse: 0.0395\n",
      "Epoch 1/10\n",
      "2558/2558 [==============================] - 77s 30ms/step - loss: 0.1180 - mae: 0.0677 - mse: 0.0338\n",
      "Epoch 2/10\n",
      "2558/2558 [==============================] - 76s 30ms/step - loss: 0.1175 - mae: 0.0675 - mse: 0.0337\n",
      "Epoch 3/10\n",
      "2558/2558 [==============================] - 77s 30ms/step - loss: 0.1171 - mae: 0.0673 - mse: 0.0336\n",
      "Epoch 4/10\n",
      "2558/2558 [==============================] - 79s 31ms/step - loss: 0.1167 - mae: 0.0670 - mse: 0.0335\n",
      "Epoch 5/10\n",
      "2558/2558 [==============================] - 80s 31ms/step - loss: 0.1165 - mae: 0.0669 - mse: 0.0335\n",
      "Epoch 6/10\n",
      "2558/2558 [==============================] - 80s 31ms/step - loss: 0.1162 - mae: 0.0668 - mse: 0.0334\n",
      "Epoch 7/10\n",
      "2558/2558 [==============================] - 79s 31ms/step - loss: 0.1160 - mae: 0.0667 - mse: 0.0333\n",
      "Epoch 8/10\n",
      "2558/2558 [==============================] - 79s 31ms/step - loss: 0.1158 - mae: 0.0666 - mse: 0.0333\n",
      "Epoch 9/10\n",
      "2558/2558 [==============================] - 81s 31ms/step - loss: 0.1158 - mae: 0.0666 - mse: 0.0333\n",
      "Epoch 10/10\n",
      "2558/2558 [==============================] - 80s 31ms/step - loss: 0.1156 - mae: 0.0665 - mse: 0.0332\n"
     ]
    }
   ],
   "source": [
    "for stack in sub_data_Sx7:\n",
    "    model_4.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 35.199999999999996\n",
      "mean steps: 57.10227272727273\n",
      "median steps: 57.0\n",
      "min steps: 40.0\n",
      "max steps: 79.0\n",
      "\n",
      "success heuristic (%): 97.2 56.26543209876543\n",
      "mean steps: 56.26543209876543\n",
      "median steps: 56.0\n",
      "min steps: 34.0\n",
      "max steps: 76.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.199999999999996, 97.2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cpmp_ml.validations import validate_model\n",
    "from cpmp_ml.optimizer import GreedyV2\n",
    "from cpmp_ml.utils.adapters import AttentionModel\n",
    "\n",
    "validate_model(model_4, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 7\n",
    "key_dim = 64\n",
    "value_dim = None\n",
    "num_heads = 3\n",
    "num_stacks = 3\n",
    "list_neurons_hide = [H+1, H+1]\n",
    "list_neurons_feed = [H+1, (H+1)*4, (H+1)*3, (H+1)*2]\n",
    "dropout = 0\n",
    "rate = 0.2\n",
    "activation_hide = 'sigmoid'\n",
    "activation_feed = 'sigmoid'\n",
    "n_dropout_hide = 0\n",
    "n_dropout_feed = 2\n",
    "\n",
    "model_5 = create_model(H=H,\n",
    "                     key_dim=key_dim,\n",
    "                     value_dim=value_dim,\n",
    "                     num_heads=num_heads,\n",
    "                     num_stacks=num_stacks,\n",
    "                     list_neurons_hide=list_neurons_hide,\n",
    "                     list_neurons_feed=list_neurons_feed,\n",
    "                     dropout=dropout,\n",
    "                     rate=rate,\n",
    "                     activation_hide=activation_hide,\n",
    "                     activation_feed=activation_feed,\n",
    "                     n_dropout_hide=n_dropout_hide,\n",
    "                     n_dropout_feed=n_dropout_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1973/1973 [==============================] - 68s 28ms/step - loss: 0.1914 - mae: 0.1127 - mse: 0.0558\n",
      "Epoch 2/10\n",
      "1973/1973 [==============================] - 57s 29ms/step - loss: 0.1660 - mae: 0.0999 - mse: 0.0497\n",
      "Epoch 3/10\n",
      "1973/1973 [==============================] - 55s 28ms/step - loss: 0.1627 - mae: 0.0979 - mse: 0.0488\n",
      "Epoch 4/10\n",
      "1973/1973 [==============================] - 56s 29ms/step - loss: 0.1609 - mae: 0.0970 - mse: 0.0483\n",
      "Epoch 5/10\n",
      "1973/1973 [==============================] - 62s 31ms/step - loss: 0.1599 - mae: 0.0963 - mse: 0.0480\n",
      "Epoch 6/10\n",
      "1973/1973 [==============================] - 64s 32ms/step - loss: 0.1591 - mae: 0.0958 - mse: 0.0478\n",
      "Epoch 7/10\n",
      "1973/1973 [==============================] - 63s 32ms/step - loss: 0.1581 - mae: 0.0952 - mse: 0.0475\n",
      "Epoch 8/10\n",
      "1973/1973 [==============================] - 63s 32ms/step - loss: 0.1575 - mae: 0.0948 - mse: 0.0473\n",
      "Epoch 9/10\n",
      "1973/1973 [==============================] - 63s 32ms/step - loss: 0.1567 - mae: 0.0944 - mse: 0.0471\n",
      "Epoch 10/10\n",
      "1973/1973 [==============================] - 63s 32ms/step - loss: 0.1564 - mae: 0.0942 - mse: 0.0470\n",
      "Epoch 1/10\n",
      "2327/2327 [==============================] - 100s 40ms/step - loss: 0.1364 - mae: 0.0804 - mse: 0.0402\n",
      "Epoch 2/10\n",
      "2327/2327 [==============================] - 93s 40ms/step - loss: 0.1357 - mae: 0.0801 - mse: 0.0400\n",
      "Epoch 3/10\n",
      "2327/2327 [==============================] - 93s 40ms/step - loss: 0.1351 - mae: 0.0798 - mse: 0.0398\n",
      "Epoch 4/10\n",
      "2327/2327 [==============================] - 93s 40ms/step - loss: 0.1348 - mae: 0.0796 - mse: 0.0397\n",
      "Epoch 5/10\n",
      "2327/2327 [==============================] - 93s 40ms/step - loss: 0.1343 - mae: 0.0793 - mse: 0.0396\n",
      "Epoch 6/10\n",
      "2327/2327 [==============================] - 91s 39ms/step - loss: 0.1340 - mae: 0.0791 - mse: 0.0395\n",
      "Epoch 7/10\n",
      "2327/2327 [==============================] - 91s 39ms/step - loss: 0.1338 - mae: 0.0790 - mse: 0.0395\n",
      "Epoch 8/10\n",
      "2327/2327 [==============================] - 92s 39ms/step - loss: 0.1335 - mae: 0.0788 - mse: 0.0394\n",
      "Epoch 9/10\n",
      "2327/2327 [==============================] - 91s 39ms/step - loss: 0.1333 - mae: 0.0787 - mse: 0.0393\n",
      "Epoch 10/10\n",
      "2327/2327 [==============================] - 92s 39ms/step - loss: 0.1330 - mae: 0.0785 - mse: 0.0393\n",
      "Epoch 1/10\n",
      "2558/2558 [==============================] - 118s 46ms/step - loss: 0.1170 - mae: 0.0672 - mse: 0.0336\n",
      "Epoch 2/10\n",
      "2558/2558 [==============================] - 119s 47ms/step - loss: 0.1165 - mae: 0.0670 - mse: 0.0335\n",
      "Epoch 3/10\n",
      "2558/2558 [==============================] - 119s 46ms/step - loss: 0.1163 - mae: 0.0668 - mse: 0.0334\n",
      "Epoch 4/10\n",
      "2558/2558 [==============================] - 116s 46ms/step - loss: 0.1159 - mae: 0.0666 - mse: 0.0333\n",
      "Epoch 5/10\n",
      "2558/2558 [==============================] - 117s 46ms/step - loss: 0.1157 - mae: 0.0665 - mse: 0.0332\n",
      "Epoch 6/10\n",
      "2558/2558 [==============================] - 118s 46ms/step - loss: 0.1154 - mae: 0.0663 - mse: 0.0332\n",
      "Epoch 7/10\n",
      "2558/2558 [==============================] - 123s 48ms/step - loss: 0.1153 - mae: 0.0662 - mse: 0.0331\n",
      "Epoch 8/10\n",
      "2558/2558 [==============================] - 115s 45ms/step - loss: 0.1152 - mae: 0.0663 - mse: 0.0331\n",
      "Epoch 9/10\n",
      "2558/2558 [==============================] - 115s 45ms/step - loss: 0.1150 - mae: 0.0661 - mse: 0.0330\n",
      "Epoch 10/10\n",
      "2558/2558 [==============================] - 115s 45ms/step - loss: 0.1149 - mae: 0.0661 - mse: 0.0330\n"
     ]
    }
   ],
   "source": [
    "for stack in sub_data_Sx7:\n",
    "    model_5.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 45.7\n",
      "mean steps: 55.61487964989059\n",
      "median steps: 55.0\n",
      "min steps: 39.0\n",
      "max steps: 77.0\n",
      "\n",
      "success heuristic (%): 96.7 56.47156153050672\n",
      "mean steps: 56.47156153050672\n",
      "median steps: 56.0\n",
      "min steps: 35.0\n",
      "max steps: 82.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45.7, 96.7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cpmp_ml.validations import validate_model\n",
    "from cpmp_ml.optimizer import GreedyV2\n",
    "from cpmp_ml.utils.adapters import AttentionModel\n",
    "\n",
    "validate_model(model_5, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_4.save('model_4.h5')\n",
    "model_5.save('model_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method StackAttention.call of <attentional_cpmp.layers.StackAttention.StackAttention object at 0x000002DE641D7590>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method StackAttention.call of <attentional_cpmp.layers.StackAttention.StackAttention object at 0x000002DE641D7590>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method FeedForward.call of <attentional_cpmp.layers.FeedForward.FeedForward object at 0x000002DE4B1AEF50>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method FeedForward.call of <attentional_cpmp.layers.FeedForward.FeedForward object at 0x000002DE4B1AEF50>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method ModelCPMP.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x000002DE657C9A80>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ModelCPMP.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x000002DE657C9A80>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "H = 7\n",
    "key_dim = 32\n",
    "value_dim = None\n",
    "num_heads = 3\n",
    "num_stacks = 3\n",
    "list_neurons_hide = [H+1, H+1]\n",
    "list_neurons_feed = [H+1, (H+1)*4, (H+1)*3, (H+1)*2]\n",
    "dropout = 0\n",
    "rate = 0.2\n",
    "activation_hide = 'sigmoid'\n",
    "activation_feed = 'sigmoid'\n",
    "n_dropout_hide = 0\n",
    "n_dropout_feed = 2\n",
    "\n",
    "model_6 = create_model(H=H,\n",
    "                     key_dim=key_dim,\n",
    "                     value_dim=value_dim,\n",
    "                     num_heads=num_heads,\n",
    "                     num_stacks=num_stacks,\n",
    "                     list_neurons_hide=list_neurons_hide,\n",
    "                     list_neurons_feed=list_neurons_feed,\n",
    "                     dropout=dropout,\n",
    "                     rate=rate,\n",
    "                     activation_hide=activation_hide,\n",
    "                     activation_feed=activation_feed,\n",
    "                     n_dropout_hide=n_dropout_hide,\n",
    "                     n_dropout_feed=n_dropout_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1973/1973 [==============================] - 68s 27ms/step - loss: 0.1837 - mae: 0.1090 - mse: 0.0542\n",
      "Epoch 2/10\n",
      "1973/1973 [==============================] - 60s 31ms/step - loss: 0.1651 - mae: 0.0991 - mse: 0.0494\n",
      "Epoch 3/10\n",
      "1973/1973 [==============================] - 67s 34ms/step - loss: 0.1621 - mae: 0.0973 - mse: 0.0485\n",
      "Epoch 4/10\n",
      "1973/1973 [==============================] - 60s 31ms/step - loss: 0.1609 - mae: 0.0966 - mse: 0.0482\n",
      "Epoch 5/10\n",
      "1973/1973 [==============================] - 50s 25ms/step - loss: 0.1597 - mae: 0.0959 - mse: 0.0478\n",
      "Epoch 6/10\n",
      "1973/1973 [==============================] - 53s 27ms/step - loss: 0.1589 - mae: 0.0955 - mse: 0.0476\n",
      "Epoch 7/10\n",
      "1973/1973 [==============================] - 60s 30ms/step - loss: 0.1583 - mae: 0.0952 - mse: 0.0474\n",
      "Epoch 8/10\n",
      "1973/1973 [==============================] - 61s 31ms/step - loss: 0.1577 - mae: 0.0948 - mse: 0.0473\n",
      "Epoch 9/10\n",
      "1973/1973 [==============================] - 62s 31ms/step - loss: 0.1572 - mae: 0.0945 - mse: 0.0472\n",
      "Epoch 10/10\n",
      "1973/1973 [==============================] - 54s 27ms/step - loss: 0.1568 - mae: 0.0943 - mse: 0.0471\n",
      "Epoch 1/10\n",
      "2327/2327 [==============================] - 74s 29ms/step - loss: 0.1366 - mae: 0.0805 - mse: 0.0402\n",
      "Epoch 2/10\n",
      "2327/2327 [==============================] - 65s 28ms/step - loss: 0.1358 - mae: 0.0801 - mse: 0.0400\n",
      "Epoch 3/10\n",
      "2327/2327 [==============================] - 67s 29ms/step - loss: 0.1352 - mae: 0.0798 - mse: 0.0398\n",
      "Epoch 4/10\n",
      "2327/2327 [==============================] - 69s 30ms/step - loss: 0.1346 - mae: 0.0794 - mse: 0.0397\n",
      "Epoch 5/10\n",
      "2327/2327 [==============================] - 66s 28ms/step - loss: 0.1345 - mae: 0.0794 - mse: 0.0396\n",
      "Epoch 6/10\n",
      "2327/2327 [==============================] - 74s 32ms/step - loss: 0.1341 - mae: 0.0792 - mse: 0.0395\n",
      "Epoch 7/10\n",
      "2327/2327 [==============================] - 75s 32ms/step - loss: 0.1338 - mae: 0.0790 - mse: 0.0394\n",
      "Epoch 8/10\n",
      "2327/2327 [==============================] - 73s 31ms/step - loss: 0.1336 - mae: 0.0789 - mse: 0.0394\n",
      "Epoch 9/10\n",
      "2327/2327 [==============================] - 72s 31ms/step - loss: 0.1334 - mae: 0.0788 - mse: 0.0393\n",
      "Epoch 10/10\n",
      "2327/2327 [==============================] - 72s 31ms/step - loss: 0.1333 - mae: 0.0787 - mse: 0.0393\n",
      "Epoch 1/10\n",
      "2558/2558 [==============================] - 93s 36ms/step - loss: 0.1173 - mae: 0.0674 - mse: 0.0336\n",
      "Epoch 2/10\n",
      "2558/2558 [==============================] - 92s 36ms/step - loss: 0.1170 - mae: 0.0672 - mse: 0.0335\n",
      "Epoch 3/10\n",
      "2558/2558 [==============================] - 102s 40ms/step - loss: 0.1165 - mae: 0.0669 - mse: 0.0334\n",
      "Epoch 4/10\n",
      "2558/2558 [==============================] - 102s 40ms/step - loss: 0.1163 - mae: 0.0669 - mse: 0.0334\n",
      "Epoch 5/10\n",
      "2558/2558 [==============================] - 92s 36ms/step - loss: 0.1160 - mae: 0.0667 - mse: 0.0333\n",
      "Epoch 6/10\n",
      "2558/2558 [==============================] - 100s 39ms/step - loss: 0.1160 - mae: 0.0666 - mse: 0.0333\n",
      "Epoch 7/10\n",
      "2558/2558 [==============================] - 93s 36ms/step - loss: 0.1157 - mae: 0.0665 - mse: 0.0332\n",
      "Epoch 8/10\n",
      "2558/2558 [==============================] - 96s 37ms/step - loss: 0.1155 - mae: 0.0664 - mse: 0.0331\n",
      "Epoch 9/10\n",
      "2558/2558 [==============================] - 132s 52ms/step - loss: 0.1153 - mae: 0.0662 - mse: 0.0331\n",
      "Epoch 10/10\n",
      "2558/2558 [==============================] - 130s 51ms/step - loss: 0.1152 - mae: 0.0662 - mse: 0.0331\n"
     ]
    }
   ],
   "source": [
    "for stack in sub_data_Sx7:\n",
    "    model_6.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 68.5\n",
      "mean steps: 21.817518248175183\n",
      "median steps: 22.0\n",
      "min steps: 7.0\n",
      "max steps: 37.0\n",
      "\n",
      "success heuristic (%): 100.0 21.515\n",
      "mean steps: 21.515\n",
      "median steps: 21.0\n",
      "min steps: 8.0\n",
      "max steps: 42.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(68.5, 100.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cpmp_ml.validations import validate_model\n",
    "from cpmp_ml.optimizer import GreedyV2\n",
    "from cpmp_ml.utils.adapters import AttentionModel\n",
    "\n",
    "validate_model(model_6, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=5, H=7, N=20, size_states=1000,\n",
    "               max_steps=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_6.save('model_6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 7\n",
    "key_dim = 8\n",
    "value_dim = None\n",
    "num_heads = 3\n",
    "num_stacks = 3\n",
    "list_neurons_hide = [H+1, H+1]\n",
    "list_neurons_feed = [H+1, (H+1)*4, (H+1)*3, (H+1)*2]\n",
    "dropout = 0\n",
    "rate = 0.2\n",
    "activation_hide = 'sigmoid'\n",
    "activation_feed = 'sigmoid'\n",
    "n_dropout_hide = 0\n",
    "n_dropout_feed = 2\n",
    "\n",
    "model_7 = create_model(H=H,\n",
    "                     key_dim=key_dim,\n",
    "                     value_dim=value_dim,\n",
    "                     num_heads=num_heads,\n",
    "                     num_stacks=num_stacks,\n",
    "                     list_neurons_hide=list_neurons_hide,\n",
    "                     list_neurons_feed=list_neurons_feed,\n",
    "                     dropout=dropout,\n",
    "                     rate=rate,\n",
    "                     activation_hide=activation_hide,\n",
    "                     activation_feed=activation_feed,\n",
    "                     n_dropout_hide=n_dropout_hide,\n",
    "                     n_dropout_feed=n_dropout_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2327/2327 [==============================] - 60s 26ms/step - loss: 0.1313 - mae: 0.0773 - mse: 0.0387\n",
      "Epoch 2/10\n",
      "2327/2327 [==============================] - 62s 27ms/step - loss: 0.1300 - mae: 0.0767 - mse: 0.0384\n",
      "Epoch 3/10\n",
      "2327/2327 [==============================] - 59s 25ms/step - loss: 0.1297 - mae: 0.0765 - mse: 0.0383\n",
      "Epoch 4/10\n",
      "2327/2327 [==============================] - 61s 26ms/step - loss: 0.1294 - mae: 0.0764 - mse: 0.0382\n",
      "Epoch 5/10\n",
      "2327/2327 [==============================] - 61s 26ms/step - loss: 0.1292 - mae: 0.0763 - mse: 0.0382\n",
      "Epoch 6/10\n",
      "2327/2327 [==============================] - 66s 28ms/step - loss: 0.1291 - mae: 0.0762 - mse: 0.0381\n",
      "Epoch 7/10\n",
      "2327/2327 [==============================] - 66s 28ms/step - loss: 0.1289 - mae: 0.0762 - mse: 0.0381\n",
      "Epoch 8/10\n",
      "2327/2327 [==============================] - 69s 30ms/step - loss: 0.1288 - mae: 0.0761 - mse: 0.0381\n",
      "Epoch 9/10\n",
      "2327/2327 [==============================] - 68s 29ms/step - loss: 0.1287 - mae: 0.0760 - mse: 0.0380\n",
      "Epoch 10/10\n",
      "2327/2327 [==============================] - 63s 27ms/step - loss: 0.1286 - mae: 0.0760 - mse: 0.0380\n",
      "Epoch 1/10\n",
      "2971/2971 [==============================] - 124s 42ms/step - loss: 0.0909 - mae: 0.0499 - mse: 0.0250\n",
      "Epoch 2/10\n",
      "2971/2971 [==============================] - 124s 42ms/step - loss: 0.0905 - mae: 0.0497 - mse: 0.0248\n",
      "Epoch 3/10\n",
      "2971/2971 [==============================] - 125s 42ms/step - loss: 0.0903 - mae: 0.0496 - mse: 0.0248\n",
      "Epoch 4/10\n",
      "2971/2971 [==============================] - 123s 42ms/step - loss: 0.0902 - mae: 0.0495 - mse: 0.0248\n",
      "Epoch 5/10\n",
      "2971/2971 [==============================] - 126s 43ms/step - loss: 0.0901 - mae: 0.0495 - mse: 0.0248\n",
      "Epoch 6/10\n",
      "2971/2971 [==============================] - 124s 42ms/step - loss: 0.0900 - mae: 0.0494 - mse: 0.0247\n",
      "Epoch 7/10\n",
      "2971/2971 [==============================] - 122s 41ms/step - loss: 0.0899 - mae: 0.0494 - mse: 0.0247\n",
      "Epoch 8/10\n",
      "2971/2971 [==============================] - 125s 42ms/step - loss: 0.0899 - mae: 0.0494 - mse: 0.0247\n",
      "Epoch 9/10\n",
      "2971/2971 [==============================] - 124s 42ms/step - loss: 0.0898 - mae: 0.0494 - mse: 0.0247\n",
      "Epoch 10/10\n",
      "2971/2971 [==============================] - 125s 42ms/step - loss: 0.0898 - mae: 0.0493 - mse: 0.0247\n",
      "Epoch 1/10\n",
      "2558/2558 [==============================] - 82s 32ms/step - loss: 0.1131 - mae: 0.0650 - mse: 0.0325\n",
      "Epoch 2/10\n",
      "2558/2558 [==============================] - 85s 33ms/step - loss: 0.1126 - mae: 0.0647 - mse: 0.0324\n",
      "Epoch 3/10\n",
      "2558/2558 [==============================] - 84s 33ms/step - loss: 0.1125 - mae: 0.0647 - mse: 0.0323\n",
      "Epoch 4/10\n",
      "2558/2558 [==============================] - 83s 32ms/step - loss: 0.1123 - mae: 0.0646 - mse: 0.0323\n",
      "Epoch 5/10\n",
      "2558/2558 [==============================] - 79s 31ms/step - loss: 0.1122 - mae: 0.0645 - mse: 0.0323\n",
      "Epoch 6/10\n",
      "2558/2558 [==============================] - 82s 32ms/step - loss: 0.1121 - mae: 0.0645 - mse: 0.0323\n",
      "Epoch 7/10\n",
      "2558/2558 [==============================] - 82s 32ms/step - loss: 0.1120 - mae: 0.0644 - mse: 0.0322\n",
      "Epoch 8/10\n",
      "2558/2558 [==============================] - 82s 32ms/step - loss: 0.1120 - mae: 0.0645 - mse: 0.0323\n",
      "Epoch 9/10\n",
      "2558/2558 [==============================] - 83s 32ms/step - loss: 0.1120 - mae: 0.0644 - mse: 0.0322\n",
      "Epoch 10/10\n",
      "2558/2558 [==============================] - 84s 33ms/step - loss: 0.1119 - mae: 0.0644 - mse: 0.0322\n",
      "Epoch 1/10\n",
      "2758/2758 [==============================] - 101s 37ms/step - loss: 0.1000 - mae: 0.0561 - mse: 0.0281\n",
      "Epoch 2/10\n",
      "2758/2758 [==============================] - 101s 37ms/step - loss: 0.0998 - mae: 0.0560 - mse: 0.0280\n",
      "Epoch 3/10\n",
      "2758/2758 [==============================] - 102s 37ms/step - loss: 0.0996 - mae: 0.0559 - mse: 0.0280\n",
      "Epoch 4/10\n",
      "2758/2758 [==============================] - 101s 37ms/step - loss: 0.0996 - mae: 0.0559 - mse: 0.0280\n",
      "Epoch 5/10\n",
      "2758/2758 [==============================] - 102s 37ms/step - loss: 0.0995 - mae: 0.0558 - mse: 0.0279\n",
      "Epoch 6/10\n",
      "2758/2758 [==============================] - 101s 37ms/step - loss: 0.0993 - mae: 0.0558 - mse: 0.0279\n",
      "Epoch 7/10\n",
      "2758/2758 [==============================] - 102s 37ms/step - loss: 0.0993 - mae: 0.0557 - mse: 0.0279\n",
      "Epoch 8/10\n",
      "2758/2758 [==============================] - 102s 37ms/step - loss: 0.0993 - mae: 0.0557 - mse: 0.0279\n",
      "Epoch 9/10\n",
      "2758/2758 [==============================] - 101s 37ms/step - loss: 0.0993 - mae: 0.0557 - mse: 0.0279\n",
      "Epoch 10/10\n",
      "2758/2758 [==============================] - 102s 37ms/step - loss: 0.0992 - mae: 0.0557 - mse: 0.0279\n",
      "Epoch 1/10\n",
      "1973/1973 [==============================] - 49s 25ms/step - loss: 0.1532 - mae: 0.0919 - mse: 0.0460\n",
      "Epoch 2/10\n",
      "1973/1973 [==============================] - 48s 25ms/step - loss: 0.1510 - mae: 0.0908 - mse: 0.0454\n",
      "Epoch 3/10\n",
      "1973/1973 [==============================] - 51s 26ms/step - loss: 0.1503 - mae: 0.0904 - mse: 0.0452\n",
      "Epoch 4/10\n",
      "1973/1973 [==============================] - 49s 25ms/step - loss: 0.1501 - mae: 0.0903 - mse: 0.0451\n",
      "Epoch 5/10\n",
      "1973/1973 [==============================] - 50s 25ms/step - loss: 0.1496 - mae: 0.0900 - mse: 0.0450\n",
      "Epoch 6/10\n",
      "1973/1973 [==============================] - 49s 25ms/step - loss: 0.1497 - mae: 0.0902 - mse: 0.0451\n",
      "Epoch 7/10\n",
      "1973/1973 [==============================] - 48s 25ms/step - loss: 0.1493 - mae: 0.0899 - mse: 0.0449\n",
      "Epoch 8/10\n",
      "1973/1973 [==============================] - 49s 25ms/step - loss: 0.1488 - mae: 0.0897 - mse: 0.0448\n",
      "Epoch 9/10\n",
      "1973/1973 [==============================] - 48s 24ms/step - loss: 0.1487 - mae: 0.0896 - mse: 0.0448\n",
      "Epoch 10/10\n",
      "1973/1973 [==============================] - 48s 25ms/step - loss: 0.1486 - mae: 0.0895 - mse: 0.0448\n",
      "Epoch 1/10\n",
      "3041/3041 [==============================] - 140s 46ms/step - loss: 0.0826 - mae: 0.0446 - mse: 0.0223\n",
      "Epoch 2/10\n",
      "3041/3041 [==============================] - 141s 46ms/step - loss: 0.0815 - mae: 0.0441 - mse: 0.0220\n",
      "Epoch 3/10\n",
      "3041/3041 [==============================] - 140s 46ms/step - loss: 0.0812 - mae: 0.0439 - mse: 0.0220\n",
      "Epoch 4/10\n",
      "3041/3041 [==============================] - 145s 48ms/step - loss: 0.0810 - mae: 0.0438 - mse: 0.0219\n",
      "Epoch 5/10\n",
      "3041/3041 [==============================] - 137s 45ms/step - loss: 0.0810 - mae: 0.0438 - mse: 0.0219\n",
      "Epoch 6/10\n",
      "3041/3041 [==============================] - 138s 46ms/step - loss: 0.0808 - mae: 0.0437 - mse: 0.0219\n",
      "Epoch 7/10\n",
      "3041/3041 [==============================] - 138s 45ms/step - loss: 0.0807 - mae: 0.0437 - mse: 0.0219\n",
      "Epoch 8/10\n",
      "3041/3041 [==============================] - 137s 45ms/step - loss: 0.0807 - mae: 0.0437 - mse: 0.0218\n",
      "Epoch 9/10\n",
      "3041/3041 [==============================] - 139s 46ms/step - loss: 0.0806 - mae: 0.0436 - mse: 0.0218\n",
      "Epoch 10/10\n",
      "3041/3041 [==============================] - 148s 49ms/step - loss: 0.0805 - mae: 0.0436 - mse: 0.0218\n"
     ]
    }
   ],
   "source": [
    "for stack in data_Sx7:\n",
    "    model_7.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 63.3\n",
      "mean steps: 54.97472353870458\n",
      "median steps: 55.0\n",
      "min steps: 36.0\n",
      "max steps: 75.0\n",
      "\n",
      "success heuristic (%): 97.8 55.95501022494888\n",
      "mean steps: 55.95501022494888\n",
      "median steps: 56.0\n",
      "min steps: 37.0\n",
      "max steps: 81.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(63.3, 97.8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cpmp_ml.validations import validate_model\n",
    "from cpmp_ml.optimizer import GreedyV2\n",
    "from cpmp_ml.utils.adapters import AttentionModel\n",
    "\n",
    "validate_model(model_6, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 71.39999999999999\n",
      "mean steps: 52.7703081232493\n",
      "median steps: 52.0\n",
      "min steps: 33.0\n",
      "max steps: 75.0\n",
      "\n",
      "success heuristic (%): 97.8 56.33742331288344\n",
      "mean steps: 56.33742331288344\n",
      "median steps: 56.0\n",
      "min steps: 35.0\n",
      "max steps: 79.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71.39999999999999, 97.8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cpmp_ml.validations import validate_model\n",
    "from cpmp_ml.optimizer import GreedyV2\n",
    "from cpmp_ml.utils.adapters import AttentionModel\n",
    "\n",
    "validate_model(model_7, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7.save('model_7.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactorizer model V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attentional_cpmp.model import create_model, load_cpmp_model\n",
    "from cpmp_ml.validations import validate_model\n",
    "from cpmp_ml.optimizer import GreedyV2\n",
    "from cpmp_ml.utils.adapters import AttentionModel\n",
    "from attentional_cpmp.utils.data_saving.data_json import load_data_from_json\n",
    "from attentional_cpmp.optimizer.BeamSearch_V2 import BeamSearch_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Sx7 = load_data_from_json(file_path=\"CPMP_With_Attention.Sx7_v4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method StackAttention.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x000001C31DB95A50>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method StackAttention.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x000001C31DB95A50>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method FeedForward.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x000001C31DB96020>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method FeedForward.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x000001C31DB96020>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method ModelCPMP.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x000001C31DB95360>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ModelCPMP.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x000001C31DB95360>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "H = 7\n",
    "key_dim = H + 1\n",
    "value_dim = None\n",
    "num_heads = 5\n",
    "num_stacks = 7\n",
    "list_neurons_hide = None\n",
    "list_neurons_feed = [(H+1)*4, (H+1)*3, (H+1)*2]\n",
    "dropout = 0\n",
    "rate = 0.2\n",
    "activation_hide = 'linear'\n",
    "activation_feed = 'sigmoid'\n",
    "n_dropout_hide = 0\n",
    "n_dropout_feed = 2\n",
    "\n",
    "model = create_model(H=H,\n",
    "                     key_dim=key_dim,\n",
    "                     value_dim=value_dim,\n",
    "                     num_heads=num_heads,\n",
    "                     num_stacks=num_stacks,\n",
    "                     list_neurons_hide=list_neurons_hide,\n",
    "                     list_neurons_feed=list_neurons_feed,\n",
    "                     dropout=dropout,\n",
    "                     rate=rate,\n",
    "                     activation_hide=activation_hide,\n",
    "                     activation_feed=activation_feed,\n",
    "                     n_dropout_hide=n_dropout_hide,\n",
    "                     n_dropout_feed=n_dropout_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2327/2327 [==============================] - 230s 85ms/step - loss: 0.1568 - mae: 0.0908 - mse: 0.0452\n",
      "Epoch 2/10\n",
      "2327/2327 [==============================] - 194s 84ms/step - loss: 0.1417 - mae: 0.0836 - mse: 0.0417\n",
      "Epoch 3/10\n",
      "2327/2327 [==============================] - 194s 83ms/step - loss: 0.1390 - mae: 0.0819 - mse: 0.0409\n",
      "Epoch 4/10\n",
      "2327/2327 [==============================] - 192s 82ms/step - loss: 0.1371 - mae: 0.0809 - mse: 0.0404\n",
      "Epoch 5/10\n",
      "2327/2327 [==============================] - 198s 85ms/step - loss: 0.1362 - mae: 0.0804 - mse: 0.0401\n",
      "Epoch 6/10\n",
      "2327/2327 [==============================] - 200s 86ms/step - loss: 0.1352 - mae: 0.0798 - mse: 0.0398\n",
      "Epoch 7/10\n",
      "2327/2327 [==============================] - 193s 83ms/step - loss: 0.1345 - mae: 0.0793 - mse: 0.0396\n",
      "Epoch 8/10\n",
      "2327/2327 [==============================] - 194s 83ms/step - loss: 0.1339 - mae: 0.0790 - mse: 0.0394\n",
      "Epoch 9/10\n",
      "2327/2327 [==============================] - 195s 84ms/step - loss: 0.1334 - mae: 0.0787 - mse: 0.0393\n",
      "Epoch 10/10\n",
      "2327/2327 [==============================] - 194s 83ms/step - loss: 0.1328 - mae: 0.0783 - mse: 0.0391\n",
      "Epoch 1/10\n",
      "2971/2971 [==============================] - 475s 154ms/step - loss: 0.0938 - mae: 0.0514 - mse: 0.0257\n",
      "Epoch 2/10\n",
      "2971/2971 [==============================] - 446s 150ms/step - loss: 0.0930 - mae: 0.0510 - mse: 0.0255\n",
      "Epoch 3/10\n",
      "2971/2971 [==============================] - 460s 155ms/step - loss: 0.0925 - mae: 0.0507 - mse: 0.0253\n",
      "Epoch 4/10\n",
      "2971/2971 [==============================] - 463s 156ms/step - loss: 0.0920 - mae: 0.0504 - mse: 0.0252\n",
      "Epoch 5/10\n",
      "2971/2971 [==============================] - 459s 155ms/step - loss: 0.0917 - mae: 0.0503 - mse: 0.0251\n",
      "Epoch 6/10\n",
      "2971/2971 [==============================] - 456s 154ms/step - loss: 0.0913 - mae: 0.0501 - mse: 0.0250\n",
      "Epoch 7/10\n",
      "2971/2971 [==============================] - 456s 153ms/step - loss: 0.0911 - mae: 0.0500 - mse: 0.0250\n",
      "Epoch 8/10\n",
      "2971/2971 [==============================] - 456s 153ms/step - loss: 0.0909 - mae: 0.0499 - mse: 0.0249\n",
      "Epoch 9/10\n",
      "2971/2971 [==============================] - 457s 154ms/step - loss: 0.0907 - mae: 0.0498 - mse: 0.0249\n",
      "Epoch 10/10\n",
      "2971/2971 [==============================] - 452s 152ms/step - loss: 0.0905 - mae: 0.0497 - mse: 0.0248\n",
      "Epoch 1/10\n",
      "2558/2558 [==============================] - 304s 119ms/step - loss: 0.1143 - mae: 0.0656 - mse: 0.0328\n",
      "Epoch 2/10\n",
      "2558/2558 [==============================] - 307s 120ms/step - loss: 0.1135 - mae: 0.0652 - mse: 0.0326\n",
      "Epoch 3/10\n",
      "2558/2558 [==============================] - 304s 119ms/step - loss: 0.1131 - mae: 0.0650 - mse: 0.0325\n",
      "Epoch 4/10\n",
      "2558/2558 [==============================] - 293s 114ms/step - loss: 0.1129 - mae: 0.0649 - mse: 0.0324\n",
      "Epoch 5/10\n",
      "2558/2558 [==============================] - 247s 97ms/step - loss: 0.1127 - mae: 0.0648 - mse: 0.0324\n",
      "Epoch 6/10\n",
      "2558/2558 [==============================] - 247s 96ms/step - loss: 0.1124 - mae: 0.0646 - mse: 0.0323\n",
      "Epoch 7/10\n",
      "2558/2558 [==============================] - 247s 97ms/step - loss: 0.1124 - mae: 0.0646 - mse: 0.0323\n",
      "Epoch 8/10\n",
      "2558/2558 [==============================] - 246s 96ms/step - loss: 0.1121 - mae: 0.0645 - mse: 0.0322\n",
      "Epoch 9/10\n",
      "2558/2558 [==============================] - 247s 97ms/step - loss: 0.1121 - mae: 0.0644 - mse: 0.0322\n",
      "Epoch 10/10\n",
      "2558/2558 [==============================] - 247s 96ms/step - loss: 0.1118 - mae: 0.0643 - mse: 0.0322\n",
      "Epoch 1/10\n",
      "2758/2758 [==============================] - 286s 104ms/step - loss: 0.1001 - mae: 0.0561 - mse: 0.0281\n",
      "Epoch 2/10\n",
      "2758/2758 [==============================] - 286s 104ms/step - loss: 0.0998 - mae: 0.0560 - mse: 0.0280\n",
      "Epoch 3/10\n",
      "2758/2758 [==============================] - 285s 103ms/step - loss: 0.0996 - mae: 0.0559 - mse: 0.0279\n",
      "Epoch 4/10\n",
      "2758/2758 [==============================] - 274s 99ms/step - loss: 0.0994 - mae: 0.0557 - mse: 0.0279\n",
      "Epoch 5/10\n",
      "2758/2758 [==============================] - 278s 101ms/step - loss: 0.0992 - mae: 0.0557 - mse: 0.0278\n",
      "Epoch 6/10\n",
      "2758/2758 [==============================] - 286s 104ms/step - loss: 0.0991 - mae: 0.0556 - mse: 0.0278\n",
      "Epoch 7/10\n",
      "2758/2758 [==============================] - 285s 103ms/step - loss: 0.0989 - mae: 0.0555 - mse: 0.0278\n",
      "Epoch 8/10\n",
      "2758/2758 [==============================] - 287s 104ms/step - loss: 0.0989 - mae: 0.0555 - mse: 0.0277\n",
      "Epoch 9/10\n",
      "2758/2758 [==============================] - 291s 106ms/step - loss: 0.0987 - mae: 0.0554 - mse: 0.0277\n",
      "Epoch 10/10\n",
      "2758/2758 [==============================] - 286s 104ms/step - loss: 0.0986 - mae: 0.0554 - mse: 0.0277\n",
      "Epoch 1/10\n",
      "1973/1973 [==============================] - 135s 68ms/step - loss: 0.1534 - mae: 0.0920 - mse: 0.0460\n",
      "Epoch 2/10\n",
      "1973/1973 [==============================] - 139s 71ms/step - loss: 0.1506 - mae: 0.0906 - mse: 0.0453\n",
      "Epoch 3/10\n",
      "1973/1973 [==============================] - 140s 71ms/step - loss: 0.1497 - mae: 0.0902 - mse: 0.0450\n",
      "Epoch 4/10\n",
      "1973/1973 [==============================] - 146s 74ms/step - loss: 0.1492 - mae: 0.0899 - mse: 0.0449\n",
      "Epoch 5/10\n",
      "1973/1973 [==============================] - 135s 68ms/step - loss: 0.1487 - mae: 0.0895 - mse: 0.0447\n",
      "Epoch 6/10\n",
      "1973/1973 [==============================] - 137s 70ms/step - loss: 0.1484 - mae: 0.0894 - mse: 0.0447\n",
      "Epoch 7/10\n",
      "1973/1973 [==============================] - 137s 69ms/step - loss: 0.1482 - mae: 0.0893 - mse: 0.0446\n",
      "Epoch 8/10\n",
      "1973/1973 [==============================] - 135s 68ms/step - loss: 0.1480 - mae: 0.0892 - mse: 0.0446\n",
      "Epoch 9/10\n",
      "1973/1973 [==============================] - 135s 69ms/step - loss: 0.1474 - mae: 0.0888 - mse: 0.0444\n",
      "Epoch 10/10\n",
      "1973/1973 [==============================] - 131s 66ms/step - loss: 0.1472 - mae: 0.0887 - mse: 0.0443\n",
      "Epoch 1/10\n",
      "3041/3041 [==============================] - 390s 128ms/step - loss: 0.0821 - mae: 0.0443 - mse: 0.0222\n",
      "Epoch 2/10\n",
      "3041/3041 [==============================] - 396s 130ms/step - loss: 0.0811 - mae: 0.0438 - mse: 0.0219\n",
      "Epoch 3/10\n",
      "3041/3041 [==============================] - 385s 127ms/step - loss: 0.0807 - mae: 0.0437 - mse: 0.0218\n",
      "Epoch 4/10\n",
      "3041/3041 [==============================] - 381s 125ms/step - loss: 0.0804 - mae: 0.0435 - mse: 0.0217\n",
      "Epoch 5/10\n",
      "3041/3041 [==============================] - 386s 127ms/step - loss: 0.0802 - mae: 0.0434 - mse: 0.0217\n",
      "Epoch 6/10\n",
      "3041/3041 [==============================] - 384s 126ms/step - loss: 0.0800 - mae: 0.0433 - mse: 0.0217\n",
      "Epoch 7/10\n",
      "3041/3041 [==============================] - 385s 126ms/step - loss: 0.0799 - mae: 0.0433 - mse: 0.0216\n",
      "Epoch 8/10\n",
      "3041/3041 [==============================] - 386s 127ms/step - loss: 0.0798 - mae: 0.0433 - mse: 0.0216\n",
      "Epoch 9/10\n",
      "3041/3041 [==============================] - 384s 126ms/step - loss: 0.0798 - mae: 0.0432 - mse: 0.0216\n",
      "Epoch 10/10\n",
      "3041/3041 [==============================] - 383s 126ms/step - loss: 0.0796 - mae: 0.0431 - mse: 0.0216\n"
     ]
    }
   ],
   "source": [
    "for stack in data_Sx7:\n",
    "    model.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 66.9\n",
      "mean steps: 54.21674140508221\n",
      "median steps: 54.0\n",
      "min steps: 33.0\n",
      "max steps: 76.0\n",
      "\n",
      "success heuristic (%): 97.6 55.97540983606557\n",
      "mean steps: 55.97540983606557\n",
      "median steps: 56.0\n",
      "min steps: 38.0\n",
      "max steps: 86.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66.9, 97.6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(model, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stack in data_Sx7:\n",
    "    model.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['5', '6', '7']) dict_keys(['8', '9', '10'])\n"
     ]
    }
   ],
   "source": [
    "claves_a_incluir_1 = ['5', '6', '7']\n",
    "claves_a_incluir_2 = ['8', '9', '10']\n",
    "\n",
    "# Crear subdiccionario\n",
    "sub_data_Sx7_1 = dict((clave, data_Sx7[clave]) for clave in claves_a_incluir_1)\n",
    "sub_data_Sx7_2 = dict((clave, data_Sx7[clave]) for clave in claves_a_incluir_2)\n",
    "\n",
    "print(sub_data_Sx7_1.keys(),sub_data_Sx7_2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1973/1973 [==============================] - 106s 54ms/step - loss: 0.1525 - mae: 0.0917 - mse: 0.0458\n",
      "Epoch 2/10\n",
      "1973/1973 [==============================] - 111s 56ms/step - loss: 0.1493 - mae: 0.0899 - mse: 0.0449\n",
      "Epoch 3/10\n",
      "1973/1973 [==============================] - 112s 57ms/step - loss: 0.1483 - mae: 0.0893 - mse: 0.0446\n",
      "Epoch 4/10\n",
      "1973/1973 [==============================] - 112s 57ms/step - loss: 0.1478 - mae: 0.0891 - mse: 0.0445\n",
      "Epoch 5/10\n",
      "1973/1973 [==============================] - 118s 60ms/step - loss: 0.1473 - mae: 0.0888 - mse: 0.0444\n",
      "Epoch 6/10\n",
      "1973/1973 [==============================] - 113s 57ms/step - loss: 0.1468 - mae: 0.0885 - mse: 0.0442\n",
      "Epoch 7/10\n",
      "1973/1973 [==============================] - 113s 57ms/step - loss: 0.1466 - mae: 0.0884 - mse: 0.0442\n",
      "Epoch 8/10\n",
      "1973/1973 [==============================] - 113s 58ms/step - loss: 0.1464 - mae: 0.0883 - mse: 0.0441\n",
      "Epoch 9/10\n",
      "1973/1973 [==============================] - 113s 57ms/step - loss: 0.1461 - mae: 0.0882 - mse: 0.0440\n",
      "Epoch 10/10\n",
      "1973/1973 [==============================] - 112s 57ms/step - loss: 0.1459 - mae: 0.0880 - mse: 0.0440\n",
      "Epoch 1/10\n",
      "2327/2327 [==============================] - 155s 67ms/step - loss: 0.1279 - mae: 0.0755 - mse: 0.0377\n",
      "Epoch 2/10\n",
      "2327/2327 [==============================] - 149s 64ms/step - loss: 0.1271 - mae: 0.0750 - mse: 0.0375\n",
      "Epoch 3/10\n",
      "2327/2327 [==============================] - 153s 66ms/step - loss: 0.1267 - mae: 0.0748 - mse: 0.0374\n",
      "Epoch 4/10\n",
      "2327/2327 [==============================] - 155s 67ms/step - loss: 0.1266 - mae: 0.0748 - mse: 0.0374\n",
      "Epoch 5/10\n",
      "2327/2327 [==============================] - 155s 67ms/step - loss: 0.1263 - mae: 0.0746 - mse: 0.0373\n",
      "Epoch 6/10\n",
      "2327/2327 [==============================] - 155s 66ms/step - loss: 0.1261 - mae: 0.0745 - mse: 0.0372\n",
      "Epoch 7/10\n",
      "2327/2327 [==============================] - 156s 67ms/step - loss: 0.1260 - mae: 0.0745 - mse: 0.0372\n",
      "Epoch 8/10\n",
      "2327/2327 [==============================] - 156s 67ms/step - loss: 0.1257 - mae: 0.0743 - mse: 0.0371\n",
      "Epoch 9/10\n",
      "2327/2327 [==============================] - 155s 66ms/step - loss: 0.1256 - mae: 0.0742 - mse: 0.0371\n",
      "Epoch 10/10\n",
      "2327/2327 [==============================] - 155s 67ms/step - loss: 0.1255 - mae: 0.0742 - mse: 0.0371\n",
      "Epoch 1/10\n",
      "2558/2558 [==============================] - 201s 79ms/step - loss: 0.1111 - mae: 0.0639 - mse: 0.0319\n",
      "Epoch 2/10\n",
      "2558/2558 [==============================] - 205s 80ms/step - loss: 0.1107 - mae: 0.0637 - mse: 0.0318\n",
      "Epoch 3/10\n",
      "2558/2558 [==============================] - 203s 79ms/step - loss: 0.1103 - mae: 0.0635 - mse: 0.0317\n",
      "Epoch 4/10\n",
      "2558/2558 [==============================] - 204s 80ms/step - loss: 0.1101 - mae: 0.0634 - mse: 0.0317\n",
      "Epoch 5/10\n",
      "2558/2558 [==============================] - 213s 83ms/step - loss: 0.1099 - mae: 0.0633 - mse: 0.0317\n",
      "Epoch 6/10\n",
      "2558/2558 [==============================] - 200s 78ms/step - loss: 0.1098 - mae: 0.0633 - mse: 0.0316\n",
      "Epoch 7/10\n",
      "2558/2558 [==============================] - 202s 79ms/step - loss: 0.1097 - mae: 0.0632 - mse: 0.0316\n",
      "Epoch 8/10\n",
      "2558/2558 [==============================] - 202s 79ms/step - loss: 0.1094 - mae: 0.0630 - mse: 0.0315\n",
      "Epoch 9/10\n",
      "2558/2558 [==============================] - 201s 79ms/step - loss: 0.1095 - mae: 0.0631 - mse: 0.0315\n",
      "Epoch 10/10\n",
      "2558/2558 [==============================] - 201s 79ms/step - loss: 0.1094 - mae: 0.0630 - mse: 0.0315\n"
     ]
    }
   ],
   "source": [
    "for stack in sub_data_Sx7_1:\n",
    "    model.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 85.3\n",
      "mean steps: 50.72801875732708\n",
      "median steps: 51.0\n",
      "min steps: 33.0\n",
      "max steps: 70.0\n",
      "\n",
      "success heuristic (%): 98.2 56.079429735234214\n",
      "mean steps: 56.079429735234214\n",
      "median steps: 56.0\n",
      "min steps: 34.0\n",
      "max steps: 88.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(85.3, 98.2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(model, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2758/2758 [==============================] - 245s 89ms/step - loss: 0.0983 - mae: 0.0552 - mse: 0.0276\n",
      "Epoch 2/10\n",
      "2758/2758 [==============================] - 244s 89ms/step - loss: 0.0979 - mae: 0.0550 - mse: 0.0275\n",
      "Epoch 3/10\n",
      "2758/2758 [==============================] - 243s 88ms/step - loss: 0.0976 - mae: 0.0548 - mse: 0.0274\n",
      "Epoch 4/10\n",
      "2758/2758 [==============================] - 242s 88ms/step - loss: 0.0975 - mae: 0.0548 - mse: 0.0274\n",
      "Epoch 5/10\n",
      "2758/2758 [==============================] - 243s 88ms/step - loss: 0.0974 - mae: 0.0547 - mse: 0.0274\n",
      "Epoch 6/10\n",
      "2758/2758 [==============================] - 242s 88ms/step - loss: 0.0972 - mae: 0.0547 - mse: 0.0273\n",
      "Epoch 7/10\n",
      "2758/2758 [==============================] - 244s 88ms/step - loss: 0.0971 - mae: 0.0546 - mse: 0.0273\n",
      "Epoch 8/10\n",
      "2758/2758 [==============================] - 240s 87ms/step - loss: 0.0970 - mae: 0.0546 - mse: 0.0273\n",
      "Epoch 9/10\n",
      "2758/2758 [==============================] - 245s 89ms/step - loss: 0.0969 - mae: 0.0545 - mse: 0.0273\n",
      "Epoch 10/10\n",
      "2758/2758 [==============================] - 242s 88ms/step - loss: 0.0968 - mae: 0.0545 - mse: 0.0272\n",
      "Epoch 1/10\n",
      "2971/2971 [==============================] - 302s 102ms/step - loss: 0.0882 - mae: 0.0485 - mse: 0.0243\n",
      "Epoch 2/10\n",
      "2971/2971 [==============================] - 302s 102ms/step - loss: 0.0880 - mae: 0.0484 - mse: 0.0242\n",
      "Epoch 3/10\n",
      "2971/2971 [==============================] - 303s 102ms/step - loss: 0.0877 - mae: 0.0483 - mse: 0.0241\n",
      "Epoch 4/10\n",
      "2971/2971 [==============================] - 304s 102ms/step - loss: 0.0876 - mae: 0.0482 - mse: 0.0241\n",
      "Epoch 5/10\n",
      "2971/2971 [==============================] - 303s 102ms/step - loss: 0.0875 - mae: 0.0482 - mse: 0.0241\n",
      "Epoch 6/10\n",
      "2971/2971 [==============================] - 304s 102ms/step - loss: 0.0874 - mae: 0.0481 - mse: 0.0240\n",
      "Epoch 7/10\n",
      "2971/2971 [==============================] - 304s 102ms/step - loss: 0.0873 - mae: 0.0481 - mse: 0.0240\n",
      "Epoch 8/10\n",
      "2971/2971 [==============================] - 303s 102ms/step - loss: 0.0874 - mae: 0.0481 - mse: 0.0240\n",
      "Epoch 9/10\n",
      "2971/2971 [==============================] - 302s 102ms/step - loss: 0.0873 - mae: 0.0481 - mse: 0.0240\n",
      "Epoch 10/10\n",
      "2971/2971 [==============================] - 305s 103ms/step - loss: 0.0870 - mae: 0.0479 - mse: 0.0240\n",
      "Epoch 1/10\n",
      "3041/3041 [==============================] - 346s 114ms/step - loss: 0.0793 - mae: 0.0430 - mse: 0.0215\n",
      "Epoch 2/10\n",
      "3041/3041 [==============================] - 343s 113ms/step - loss: 0.0792 - mae: 0.0429 - mse: 0.0214\n",
      "Epoch 3/10\n",
      "3041/3041 [==============================] - 345s 113ms/step - loss: 0.0789 - mae: 0.0428 - mse: 0.0214\n",
      "Epoch 4/10\n",
      "3041/3041 [==============================] - 345s 113ms/step - loss: 0.0789 - mae: 0.0428 - mse: 0.0214\n",
      "Epoch 5/10\n",
      "3041/3041 [==============================] - 344s 113ms/step - loss: 0.0787 - mae: 0.0427 - mse: 0.0213\n",
      "Epoch 6/10\n",
      "3041/3041 [==============================] - 343s 113ms/step - loss: 0.0787 - mae: 0.0427 - mse: 0.0213\n",
      "Epoch 7/10\n",
      "3041/3041 [==============================] - 344s 113ms/step - loss: 0.0786 - mae: 0.0426 - mse: 0.0213\n",
      "Epoch 8/10\n",
      "3041/3041 [==============================] - 344s 113ms/step - loss: 0.0784 - mae: 0.0425 - mse: 0.0213\n",
      "Epoch 9/10\n",
      "3041/3041 [==============================] - 343s 113ms/step - loss: 0.0784 - mae: 0.0426 - mse: 0.0213\n",
      "Epoch 10/10\n",
      "3041/3041 [==============================] - 344s 113ms/step - loss: 0.0784 - mae: 0.0425 - mse: 0.0213\n"
     ]
    }
   ],
   "source": [
    "for stack in sub_data_Sx7_2:\n",
    "    model.fit(data_Sx7[stack][\"States\"], data_Sx7[stack][\"Labels\"], batch_size= 32, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success ann model (%): 92.10000000000001\n",
      "mean steps: 49.91965255157437\n",
      "median steps: 50.0\n",
      "min steps: 33.0\n",
      "max steps: 65.0\n",
      "\n",
      "success heuristic (%): 97.89999999999999 56.31460674157304\n",
      "mean steps: 56.31460674157304\n",
      "median steps: 56.0\n",
      "min steps: 37.0\n",
      "max steps: 84.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92.10000000000001, 97.89999999999999)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(model, optimizer=GreedyV2(), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method StackAttention.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x00000271A9CC3880>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method StackAttention.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x00000271A9CC3880>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method FeedForward.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x00000271A9CC3DF0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method FeedForward.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x00000271A9CC3DF0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method ModelCPMP.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x00000271A9CC3190>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ModelCPMP.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x00000271A9CC3190>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "model_test = load_cpmp_model('model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBeamSearch_V2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAttentionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAttentionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cpmp_ml\\validations\\metrics.py:19\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, optimizer, data_adapter, S, H, N, size_states, benchmark_csv, **kwargs)\u001b[0m\n\u001b[0;32m     17\u001b[0m lays1 \u001b[38;5;241m=\u001b[39m deepcopy(lays)\n\u001b[0;32m     18\u001b[0m costs1 \u001b[38;5;241m=\u001b[39m optimizer_model\u001b[38;5;241m.\u001b[39msolve(np\u001b[38;5;241m.\u001b[39marray(lays1), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m costs2 \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlays\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     21\u001b[0m valid_costs1 \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m costs1 \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     22\u001b[0m valid_costs2 \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m costs2 \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\thoma\\OneDrive\\Documentos\\Projects\\Repositorio\\VAntigua\\CPMP_With_attention-dev-Slinking196\\attentional_cpmp\\optimizer\\BeamSearch_V2.py:230\u001b[0m, in \u001b[0;36mBeamSearch_V2.solve\u001b[1;34m(self, lays, **kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m session_lays, pred_lays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__verify_solutions__(session_lays, costs, steps, pred_lays)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(session_lays) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_steps: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m child_session_lays, pred_child_lays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__expand_lays__\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_lays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m multiply_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__multiply_predictions__(pred_lays, pred_child_lays)\n\u001b[0;32m    232\u001b[0m session_lays, pred_lays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__best_moves__(child_session_lays, pred_child_lays, multiply_pred)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\OneDrive\\Documentos\\Projects\\Repositorio\\VAntigua\\CPMP_With_attention-dev-Slinking196\\attentional_cpmp\\optimizer\\BeamSearch_V2.py:96\u001b[0m, in \u001b[0;36mBeamSearch_V2.__expand_lays__\u001b[1;34m(self, session_lays)\u001b[0m\n\u001b[0;32m     94\u001b[0m child_session_lays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m     95\u001b[0m child_session_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m---> 96\u001b[0m model_lays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_model_lays__\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_lays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m pred_lays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__model\u001b[38;5;241m.\u001b[39mpredict(model_lays, verbose\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     99\u001b[0m K\u001b[38;5;241m.\u001b[39mclear_session()\n",
      "File \u001b[1;32mc:\\Users\\thoma\\OneDrive\\Documentos\\Projects\\Repositorio\\VAntigua\\CPMP_With_attention-dev-Slinking196\\attentional_cpmp\\optimizer\\BeamSearch_V2.py:73\u001b[0m, in \u001b[0;36mBeamSearch_V2.__get_model_lays__\u001b[1;34m(self, session_lays)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(states_size):\n\u001b[0;32m     71\u001b[0m         model_lays\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data_adapter\u001b[38;5;241m.\u001b[39mget_ann_state(states[i]))\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_lays\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\shape_base.py:445\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[0;32m    443\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    447\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "validate_model(model_test, optimizer=BeamSearch_V2(model_test, AttentionModel(), 7, 100, 0.5), data_adapter=AttentionModel(), S=10, H=7, N=50, size_states=1000,\n",
    "               max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from attentional_cpmp.model import load_cpmp_model\n",
    "from attentional_cpmp.validations import validate_optimizers\n",
    "\n",
    "from cpmp_ml.optimizer import GreedyV2, GreedyModel\n",
    "from cpmp_ml.utils.adapters import AttentionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method StackAttention.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x0000022774473E50>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method StackAttention.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x0000022774473E50>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method FeedForward.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x00000227744738E0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method FeedForward.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x00000227744738E0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method ModelCPMP.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x00000227744723B0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ModelCPMP.call of <tensorflow.python.eager.polymorphic_function.tf_method_target.TfMethodTarget object at 0x00000227744723B0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "model = load_cpmp_model('.\\models\\model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Optimizer 1 ****************\n",
      "Number of problems solved: 9294/10000\n",
      "Mean steps: 12.176888315041962\n",
      "Median steps: 12.0\n",
      "Min steps: 1.0\n",
      "Max steps: 27.0\n",
      "\n",
      "************ Optimizer 2 ****************\n",
      "Number of problems solved: 9294/10000\n",
      "Mean steps: 11.758015924252206\n",
      "Median steps: 12.0\n",
      "Min steps: 1.0\n",
      "Max steps: 27.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "greedy_1 = GreedyModel(model=model, data_adapter=AttentionModel())\n",
    "greedy_2 = GreedyV2()\n",
    "optimizers = [greedy_1, greedy_2]\n",
    "validate_optimizers(optimizers=optimizers,\n",
    "                    S=5,\n",
    "                    H=7,\n",
    "                    N=15,\n",
    "                    sample_size=10000,\n",
    "                    calculate_only_solved=True,\n",
    "                    max_steps=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'key_dim': 8, 'value_dim': None, 'num_heads': 5, 'num_stacks': 7, 'list_neurons_hide': None, 'list_neurons_feed': [32, 24, 16], 'dropout': None, 'rate': 0.2, 'activation_hide': 'linear', 'activation_feed': 'sigmoid', 'n_dropout_hide': 0, 'n_dropout_feed': 2}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('good_params.json', 'r') as archivo:\n",
    "      params = json.load(archivo)\n",
    "\n",
    "print(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
